{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ca5932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ab9a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 1\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon_product(product_name):\n",
    "    url = f\"https://www.amazon.in/s?k={product_name}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        product_list = soup.find_all(\"span\", {\"class\": \"a-size-medium\"})\n",
    "        if product_list:\n",
    "            print(\"Products found:\")\n",
    "            for product in product_list:\n",
    "                print(product.text)\n",
    "        else:\n",
    "            print(\"No products found.\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve data from Amazon.\")\n",
    "\n",
    "user_input = input(\"Enter the product to search on Amazon: \")\n",
    "search_amazon_product(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303afd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 2\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_product_details(product_name):\n",
    "    url = f\"https://www.amazon.in/s?k={product_name}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        products = soup.find_all(\"div\", class_=\"s-asin\")\n",
    "        data = []\n",
    "        for product in products:\n",
    "            details = {}\n",
    "            try:\n",
    "                details[\"Brand Name\"] = product.find(\"span\", class_=\"a-size-base-plus\").text.strip()\n",
    "            except AttributeError:\n",
    "                details[\"Brand Name\"] = \"-\"\n",
    "            try:\n",
    "                details[\"Name of the Product\"] = product.find(\"span\", class_=\"a-text-normal\").text.strip()\n",
    "            except AttributeError:\n",
    "                details[\"Name of the Product\"] = \"-\"\n",
    "            try:\n",
    "                details[\"Price\"] = product.find(\"span\", class_=\"a-offscreen\").text.strip()\n",
    "            except AttributeError:\n",
    "                details[\"Price\"] = \"-\"\n",
    "            try:\n",
    "                details[\"Return/Exchange\"] = product.find(\"span\", class_=\"a-text-bold\").text.strip()\n",
    "            except AttributeError:\n",
    "                details[\"Return/Exchange\"] = \"-\"\n",
    "            try:\n",
    "                details[\"Expected Delivery\"] = product.find(\"span\", class_=\"a-text-bold\").find_next_sibling(\"span\").text.strip()\n",
    "            except AttributeError:\n",
    "                details[\"Expected Delivery\"] = \"-\"\n",
    "            try:\n",
    "                details[\"Availability\"] = product.find(\"span\", class_=\"a-size-base\").text.strip()\n",
    "            except AttributeError:\n",
    "                details[\"Availability\"] = \"-\"\n",
    "            try:\n",
    "                details[\"Product URL\"] = \"https://www.amazon.in\" + product.find(\"a\", class_=\"a-link-normal\")[\"href\"]\n",
    "            except AttributeError:\n",
    "                details[\"Product URL\"] = \"-\"\n",
    "            data.append(details)\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(f\"{product_name}_products.csv\", index=False)\n",
    "        print(\"Data saved to CSV file.\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve data from Amazon.\")\n",
    "\n",
    "user_input = input(\"Enter the product to search on Amazon: \")\n",
    "scrape_product_details(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fc2f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 3\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_google_images(keywords, num_images):\n",
    "    base_url = \"https://www.google.com/search?q={}&tbm=isch\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        url = base_url.format(keyword)\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            image_tags = soup.find_all(\"img\", class_=\"t0fcAb\")\n",
    "            print(f\"Scraping images for keyword: {keyword}\")\n",
    "            for i, img in enumerate(image_tags[:num_images]):\n",
    "                img_url = img[\"src\"]\n",
    "                img_name = f\"{keyword}_{i+1}.jpg\"\n",
    "                with open(img_name, \"wb\") as f:\n",
    "                    f.write(requests.get(img_url).content)\n",
    "                print(f\"Image {i+1} saved as {img_name}\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve images for keyword: {keyword}\")\n",
    "\n",
    "keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "num_images = 10\n",
    "scrape_google_images(keywords,Â num_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85135345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_flipkart_smartphones(search_query):\n",
    "    url = f\"https://www.flipkart.com/search?q={search_query}&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        products = soup.find_all(\"div\", class_=\"_1AtVbE\")\n",
    "        data = []\n",
    "        for product in products:\n",
    "            details = {}\n",
    "            try:\n",
    "                details[\"Brand Name\"] = product.find(\"div\", class_=\"_4rR01T\").text.strip()\n",
    "            except AttributeError:\n",
    "                details[\"Brand Name\"] = \"-\"\n",
    "            try:\n",
    "                details[\"Smartphone name\"] = product.find(\"a\", class_=\"IRpwTa\").text.strip()\n",
    "            except AttributeError:\n",
    "                details[\"Smartphone name\"] = \"-\"\n",
    "            try:\n",
    "                details[\"Colour\"] = product.find(\"div\", class_=\"_4rR01T\").text.strip()\n",
    "            except AttributeError:\n",
    "                details[\"Colour\"] = \"-\"\n",
    "            try:\n",
    "                details[\"RAM\"] = product.find_all(\"li\", class_=\"rgWa7D\")[0].text.strip()\n",
    "            except IndexError:\n",
    "                details[\"RAM\"] = \"-\"\n",
    "            try:\n",
    "                details[\"Storage(ROM)\"] = product.find_all(\"li\", class_=\"rgWa7D\")[1].text.strip()\n",
    "            except IndexError:\n",
    "                details[\"Storage(ROM)\"] = \"-\"\n",
    "            try:\n",
    "                details[\"Primary Camera\"] = product.find_all(\"li\", class_=\"rgWa7D\")[2].text.strip()\n",
    "            except IndexError:\n",
    "                details[\"Primary Camera\"] = \"-\"\n",
    "            try:\n",
    "                details[\"Secondary Camera\"] = product.find_all(\"li\", class_=\"rgWa7D\")[3].text.strip()\n",
    "            except IndexError:\n",
    "                details[\"Secondary Camera\"] = \"-\"\n",
    "            try:\n",
    "                details[\"Display Size\"] = product.find_all(\"li\", class_=\"rgWa7D\")[4].text.strip()\n",
    "            except IndexError:\n",
    "                details[\"Display Size\"] = \"-\"\n",
    "            try:\n",
    "                details[\"Battery Capacity\"] = product.find_all(\"li\", class_=\"rgWa7D\")[5].text.strip()\n",
    "            except IndexError:\n",
    "                details[\"Battery Capacity\"] = \"-\"\n",
    "            try:\n",
    "                details[\"Price\"] = product.find(\"div\", class_=\"_30jeq3\").text.strip()\n",
    "            except AttributeError:\n",
    "                details[\"Price\"] = \"-\"\n",
    "            try:\n",
    "                details[\"Product URL\"] = \"https://www.flipkart.com\" + product.find(\"a\", class_=\"IRpwTa\")[\"href\"]\n",
    "            except AttributeError:\n",
    "                details[\"Product URL\"] = \"-\"\n",
    "            data.append(details)\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(f\"{search_query}_products.csv\", index=False)\n",
    "        print(\"Data saved to CSV file.\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve data from Flipkart.\")\n",
    "\n",
    "search_query = input(\"Enter the smartphone to search on Flipkart: \")\n",
    "scrape_flipkart_smartphones(search_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65c9197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 5\n",
    "import requests\n",
    "\n",
    "def get_coordinates(city_name):\n",
    "    base_url = \"https://maps.googleapis.com/maps/api/geocode/json\"\n",
    "    params = {\n",
    "        \"address\": city_name,\n",
    "        \"key\": \"YOUR_API_KEY\"  # Replace \"YOUR_API_KEY\" with your actual Google Maps API key\n",
    "    }\n",
    "    response = requests.get(base_url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if data[\"status\"] == \"OK\":\n",
    "            location = data[\"results\"][0][\"geometry\"][\"location\"]\n",
    "            latitude = location[\"lat\"]\n",
    "            longitude = location[\"lng\"]\n",
    "            print(f\"Coordinates for {city_name}: Latitude {latitude}, Longitude {longitude}\")\n",
    "        else:\n",
    "            print(\"Failed to retrieve coordinates. Please check your input.\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve data from Google Maps.\")\n",
    "\n",
    "city_name = input(\"Enter the city name to get its coordinates: \")\n",
    "get_coordinates(city_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cc2465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 6\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_digit_gaming_laptops():\n",
    "    url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        laptops = soup.find_all(\"div\", class_=\"TopNumbeHeading active sticky-footer\")\n",
    "        data = []\n",
    "        for laptop in laptops:\n",
    "            details = {}\n",
    "            try:\n",
    "                details[\"Name\"] = laptop.find(\"div\", class_=\"TopNumbeHeading active sticky-footer\").text.strip()\n",
    "            except AttributeError:\n",
    "                details[\"Name\"] = \"-\"\n",
    "            try:\n",
    "                details[\"Price\"] = laptop.find(\"div\", class_=\"smprice\").text.strip()\n",
    "            except AttributeError:\n",
    "                details[\"Price\"] = \"-\"\n",
    "            try:\n",
    "                details[\"Specifications\"] = laptop.find(\"div\", class_=\"TopNumbeHeading active sticky-footer\").text.strip()\n",
    "            except AttributeError:\n",
    "                details[\"Specifications\"] = \"-\"\n",
    "            data.append(details)\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(\"gaming_laptops.csv\", index=False)\n",
    "        print(\"Data saved to CSV file.\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve data from digit.in.\")\n",
    "\n",
    "scrape_digit_gaming_laptops()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465c1eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 7\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_forbes_billionaires():\n",
    "    url = \"https://www.forbes.com/billionaires/\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        billionaires = soup.find_all(\"div\", class_=\"personName\")\n",
    "        data = []\n",
    "        for billionaire in billionaires:\n",
    "            details = {}\n",
    "            try:\n",
    "                details[\"Rank\"] = billionaire.find_previous(\"div\", class_=\"rank\").text.strip()\n",
    "            except AttributeError:\n",
    "                details[\"Rank\"] = \"-\"\n",
    "            try:\n",
    "                details[\"Name\"] = billionaire.text.strip()\n",
    "            except AttributeError:\n",
    "                details[\"Name\"] = \"-\"\n",
    "            try:\n",
    "                details[\"Net worth\"] = billionaire.find_next(\"div\", class_=\"netWorth\").text.strip()\n",
    "            except AttributeError:\n",
    "                details[\"Net worth\"] = \"-\"\n",
    "            try:\n",
    "                details[\"Age\"] = billionaire.find_next(\"div\", class_=\"age\").text.strip()\n",
    "            except AttributeError:\n",
    "                details[\"Age\"] = \"-\"\n",
    "            try:\n",
    "                details[\"Citizenship\"] = billionaire.find_next(\"div\", class_=\"countryOfCitizenship\").text.strip()\n",
    "            except AttributeError:\n",
    "                details[\"Citizenship\"] = \"-\"\n",
    "            try:\n",
    "                details[\"Source\"] = billionaire.find_next(\"div\", class_=\"source-column\").text.strip()\n",
    "            except AttributeError:\n",
    "                details[\"Source\"] = \"-\"\n",
    "            try:\n",
    "                details[\"Industry\"] = billionaire.find_next(\"div\", class_=\"category\").text.strip()\n",
    "            except AttributeError:\n",
    "                details[\"Industry\"] = \"-\"\n",
    "            data.append(details)\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(\"forbes_billionaires.csv\", index=False)\n",
    "        print(\"Data saved to CSV file.\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve data from Forbes.\")\n",
    "\n",
    "scrape_forbes_billionaires()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109a81dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 8\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# API key obtained from Google Developers Console\n",
    "API_KEY = 'YOUR_API_KEY'\n",
    "\n",
    "def get_video_comments(video_id):\n",
    "    youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "    comments = []\n",
    "\n",
    "    request = youtube.commentThreads().list(\n",
    "        part=\"snippet\",\n",
    "        videoId=video_id,\n",
    "        maxResults=100\n",
    "    )\n",
    "\n",
    "    while request:\n",
    "        response = request.execute()\n",
    "\n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']\n",
    "            comments.append({\n",
    "                'comment': comment['textDisplay'],\n",
    "                'author': comment['authorDisplayName'],\n",
    "                'votes': comment['likeCount'],\n",
    "                'timestamp': comment['publishedAt']\n",
    "            })\n",
    "\n",
    "        request = youtube.commentThreads().list_next(request, response)\n",
    "\n",
    "    return comments\n",
    "\n",
    "# Example usage\n",
    "video_id = 'VIDEO_ID'  # Replace with the ID of the YouTube video\n",
    "comments = get_video_comments(video_id)\n",
    "\n",
    "for comment in comments:\n",
    "    print(f\"Comment: {comment['comment']}\")\n",
    "    print(f\"Author: {comment['author']}\")\n",
    "    print(f\"Votes: {comment['votes']}\")\n",
    "    print(f\"Timestamp: {comment['timestamp']}\")\n",
    "    print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf950c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 9\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_hostels_in_london():\n",
    "    url = \"https://www.hostelworld.com/s?q=London,%20England&country=England&city=London&type=city&id=3&from=2024-03-25&to=2024-03-26&guests=1&page=1\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        hostels = soup.find_all(\"div\", class_=\"fab9ae92e1\")\n",
    "        data = []\n",
    "        for hostel in hostels:\n",
    "            details = {}\n",
    "            try:\n",
    "                details[\"Hostel Name\"] = hostel.find(\"h2\", class_=\"a97fb91265\").text.strip()\n",
    "            except AttributeError:\n",
    "                details[\"Hostel Name\"] = \"-\"\n",
    "            try:\n",
    "                details[\"Distance from City Center\"] = hostel.find(\"span\", class_=\"fcdc9d67d6\").text.strip()\n",
    "            except AttributeError:\n",
    "                details[\"Distance from City Center\"] = \"-\"\n",
    "            try:\n",
    "                details[\"Ratings\"] = hostel.find(\"div\", class_=\"rating rating-summary-container big\").text.strip()\n",
    "            except AttributeError:\n",
    "                details[\"Ratings\"] = \"-\"\n",
    "            try:\n",
    "                details[\"Total Reviews\"] = hostel.find(\"div\", class_=\"reviews\").text.strip()\n",
    "            except AttributeError:\n",
    "                details[\"Total Reviews\"] = \"-\"\n",
    "            try:\n",
    "                details[\"Overall Reviews\"] = hostel.find(\"span\", class_=\"rating-score\").text.strip()\n",
    "            except AttributeError:\n",
    "                details[\"Overall Reviews\"] = \"-\"\n",
    "            try:\n",
    "                details[\"Privates from price\"] = hostel.find(\"span\", class_=\"privates\").text.strip()\n",
    "            except AttributeError:\n",
    "                details[\"Privates from price\"] = \"-\"\n",
    "            try:\n",
    "                details[\"Dorms from price\"] = hostel.find(\"span\", class_=\"dorms\").text.strip()\n",
    "            except AttributeError:\n",
    "                details[\"Dorms from price\"] = \"-\"\n",
    "            try:\n",
    "                details[\"Facilities\"] = \", \".join([fac.text.strip() for fac in hostel.find_all(\"div\", class_=\"facilities\")])\n",
    "            except AttributeError:\n",
    "                details[\"Facilities\"] = \"-\"\n",
    "            try:\n",
    "                details[\"Property Description\"] = hostel.find(\"div\", class_=\"listing-description\").text.strip()\n",
    "            except AttributeError:\n",
    "                details[\"Property Description\"] = \"-\"\n",
    "            data.append(details)\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(\"london_hostels.csv\", index=False)\n",
    "        print(\"Data saved to CSV file.\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve data from Hostelworld.\")\n",
    "\n",
    "scrape_hostels_in_london()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b7c718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
